{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intensive-status",
   "metadata": {},
   "source": [
    "# Deep Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-communist",
   "metadata": {},
   "source": [
    "## Richer ; May 15th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-heart",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-surrey",
   "metadata": {},
   "source": [
    "Actually we can categorize the deep learning into the following parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-healthcare",
   "metadata": {},
   "source": [
    "|  Domain   | Detial  |\n",
    "|  ----  | ----  |\n",
    "| Images | CNN |\n",
    "| NLP | RNN, LSTM |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-forest",
   "metadata": {},
   "source": [
    "And here is a basic ideas: training scale drives deep learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-crest",
   "metadata": {},
   "source": [
    "### 1.1 Introduction to LR (logisic regression) Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-finnish",
   "metadata": {},
   "source": [
    "The loss function of LR(logisic regression) Method is below, the idea is to minimize the cost function by alternating the parameters.\n",
    "$$\\text{Pre:}\\hat{y}=\\sigma(w^{T}x+b)\\text{ where } \\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "$$\\text{Loss Fun:}L(\\hat{y},y)=-\\big(ylog\\hat{y}+(1-y)log(1-\\hat{y})\\big)$$\n",
    "$$\\text{Cost Fun:}J(w,b)=\\frac{1}{m}\\sum L(\\hat{y}^{i},y^{i})$$\n",
    "\n",
    "and **gradient descent** method is applied for $w$ and $b$ below:\n",
    "$$ w:=w-\\alpha\\frac{dJ(w)}{dt}\\text{(notationaly dw)}\n",
    "$$\n",
    "\n",
    "Intuitively, we have the forward propogation and the backward propogation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-nebraska",
   "metadata": {},
   "source": [
    "**Psudocode**: In python, we often write dv for $\\frac{\\partial J}{\\partial v}$. In follows we write $x=[x^{(1)},x^{(2)},...,x^{(m)}]$, \n",
    "$dx=[dx^{(1)},dx^{(2)},...,dx^{(m)}]$ and etc.\n",
    "For example we write \n",
    "$$ [z^{(1)},z^{(2)},...,z^{(m)}]=w^{T}[x^{(1)},x^{(2)},...,x^{(m)}]+[b,b,...,b]\n",
    "$$ \n",
    "in $z=w^{T}x+b$.\n",
    "\n",
    "1. J=0; $dw=np.zeros((n_x,1))$; db=0.\n",
    "2. For i=1 to m ( $i$ stands for number of training examples )\n",
    "<br> $\\quad  z=w^{T}x+b$\n",
    "<br> $\\quad  a=\\sigma(z)$\n",
    "<br> $\\quad  J+=-[y^{(i)}log(a^{(i)})+(1-y^{(i)}）log(1-a^{(i)})]$\n",
    "<br> $\\quad  dz=a-y$\n",
    "<br> $\\quad  dw=\\frac{1}{m}Xdz^{T}$\n",
    "<br> $\\quad  db=\\frac{1}{m}np.sum(dz)$\n",
    "3. iterate the parameter\n",
    "<br> $\\quad  w=w-\\alpha dw; b=b-\\alpha db$\n",
    "\n",
    "**Code Time**: Whenever possible, avoid explicit for-loops ( Especially it's better off if there exist built-in functions ). Therefore **Vectorization** matters when it comes to this!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-rescue",
   "metadata": {},
   "source": [
    "### 1.2 Vectorization Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-substance",
   "metadata": {},
   "source": [
    "Art of getting rid of explicit folders in code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-relief",
   "metadata": {},
   "source": [
    "**Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "historical-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(8,1) # dont use randn(8)! \n",
    "b = np.random.randn(8,1)\n",
    "c = np.dot(a.T,b)\n",
    "d = np.exp(b) # maximum/**/log is also applicable\n",
    "#print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-mount",
   "metadata": {},
   "source": [
    "**broadcasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "original-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=np.c_[a,b,d]\n",
    "f=np.dot(e.T,d)+1 # broadcasting in python!\n",
    "a=a.reshape(8,1)\n",
    "assert(a.shape==(8,1))\n",
    "g=e/a # More general broadcasting !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-louisiana",
   "metadata": {},
   "source": [
    "## 2 Implement one hidden-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-donna",
   "metadata": {},
   "source": [
    "Some assumption.\n",
    "1. superscript 2 in $a^{[2]}_{3}$ refers to the layer number and the subscript 3 refers to the corresponding node.\n",
    "2. structure is below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-package",
   "metadata": {},
   "source": [
    "**Input Layer -- Hidden Layer -- Output Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-display",
   "metadata": {},
   "source": [
    "where hidden layer means the true values are still out there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-louis",
   "metadata": {},
   "source": [
    "3. every hidden unit has two steps calculations. And gethering together we have a matrix mutiplication. First\n",
    "$$ z^{[i](j)}=w^{[i]}x^{(j)}+b^{[i]}\n",
    "$$\n",
    "$$ a^{[i](j)}=\\sigma(z^{[i](j)})\n",
    "$$\n",
    "\n",
    "Also we can vectorize. Therefore let $x=[x^{(1)},x^{(2)},..,x^{(m)}]$\n",
    "Intuitively, horizontally is training examples and vertically is the nodes.\n",
    "\n",
    "Therefore we have a more general formula\n",
    "$$ z^{[i]}=w^{[i]}x+b^{[i]}\n",
    "$$\n",
    "$$ a^{[i]}=\\sigma(z^{[i]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-petite",
   "metadata": {},
   "source": [
    "Formally, by changing x with $a^{[i-1]}$, we have\n",
    "$$ z^{[i]}=w^{[i]}a^{[i-1]}+b^{[i]}\n",
    "$$\n",
    "$$ a^{[i]}=\\sigma(z^{[i]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-produce",
   "metadata": {},
   "source": [
    "### 2.1 Activation Function and Compute of its Slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-chaos",
   "metadata": {},
   "source": [
    "Actually, $tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$ is \n",
    "better than $\\sigma(z)$ because it's more \"centerable\". **And ReLU is best and the default choice nowadays**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-capacity",
   "metadata": {},
   "source": [
    "Here are some rule of thumbs:\n",
    "1. binary classification:  $\\sigma$ -- output layer; ReLU --  others\n",
    "2. restriction of output ( e.g. predict the house price which is always nonnegative), then we can let $Relu$ -- output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-bicycle",
   "metadata": {},
   "source": [
    "We also have\n",
    "$$ \\sigma'=\\sigma(1-\\sigma) \\quad ; \\quad tanh'(z)=1-tanh^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-netherlands",
   "metadata": {},
   "source": [
    "### 2.2 Implement gradient descent for NN with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-payment",
   "metadata": {},
   "source": [
    "**Algorithm for the case when $\\hat{y}$ is a row vector**\n",
    "\n",
    "Repeat the follows!\n",
    "1. compute the product $\\hat{y}^{(i)}$.\n",
    "2. compute the partial derivative $dw^{[i]}$ and $db^{[i]}$. ( we compute it from the loss function, e.g. $L(a^{[2]},y)$ )\n",
    "3. $w^{[i]}=w^{[i]}-\\alpha dw^{[i]}$; $b^{[i]}=b^{[i]}-\\alpha db^{[i]}$; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-shift",
   "metadata": {},
   "source": [
    "Specificaly ( below we let $g^{2}=\\sigma$ ) we have two steps:\n",
    "\n",
    "**Forward Propogation**\n",
    "1. \n",
    "$$z^{[1]}=w^{[1]}x+b^{[1]}\\quad  \\text{ and }\\quad\n",
    "   a^{[1]}=g^{[1]}(z^{[1]})\n",
    "$$\n",
    "2. \n",
    "$$z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}\\quad  \\text{ and }\\quad\n",
    "   a^{[2]}=g^{[2]}(z^{[2]})=\\sigma(z^{[2]})\n",
    "$$\n",
    "\n",
    "**Backword Propogation** ( m training examples )\n",
    "1. \n",
    "$$ \\quad dz^{[2]}=a^{[2]}-y\n",
    "$$\n",
    "$$ \\quad dw^{[2]}=\\frac{1}{m}dz^{[2]}a^{[1]T}\n",
    "$$\n",
    "$$ \\quad db^{[2]}=\\frac{1}{m}np.sum(dz^{[2]},axis=1,keepdims=True)\n",
    "$$\n",
    "2. \n",
    "$$ \\quad dz^{[1]}=(dz^{[2]T}w^{[2]})^{T}\\cdot dg^{[1]}(z^{[1]})\n",
    "$$\n",
    "$$ \\quad dw^{[1]}=\\frac{1}{m}dz^{[1]}a^{[0]T}\n",
    "$$\n",
    "$$ \\quad db^{[1]}=\\frac{1}{m}np.sum(dz^{[1]},axis=1,keepdims=True)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-little",
   "metadata": {},
   "source": [
    "**Initialization**\n",
    "\n",
    "Aware to be rondomly initialize to avoid **symmetry breaking problem**, e.g.\n",
    "$$ w^{[i]}=np.random.randn([2,2])*0.01 $$\n",
    "where we multiply 0.01 to avoid learning too slow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-coral",
   "metadata": {},
   "source": [
    "## 3 Implement deep (more than one hidden-layer) Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-alignment",
   "metadata": {},
   "source": [
    "**Notation**\n",
    "1. $L=$#layers \n",
    "2. $n^{[l]}$=#units in layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-damages",
   "metadata": {},
   "source": [
    "**Formula ( vectorized: stack the whole training examples )**\n",
    "\n",
    "**Forward Propogation** $a^{[l-1]}$ to $a^{[l]}，z^{[l]}$\n",
    "\n",
    "1. $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$\n",
    "2. $a^{[l]}=g^{[l]}(z^{[l]})$\n",
    "3. $x=a^{[0]}\\in M^{(n_l,m)}\\quad \\text{and} \\quad \\hat{y}=a^{[L]}$\n",
    "\n",
    "**Backward Propogation** $da^{[l]}$ to $da^{[l-1]},dw^{[l]},db^{[l]}$\n",
    "\n",
    "1. $dz^{[l]}=da^{[l]}\\cdot dg^{[l]}(z^{[l]})$\n",
    "2. $dw^{[l]}=\\frac{1}{m}dz^{[l]}\\cdot a^{[l-1]}$\n",
    "$db^{[l]}=\\frac{1}{m}n_p.sum(dz^{[l]},axis=1,keepdims=True)$\n",
    "\n",
    "3. $da^{[l-1]}=w^{[l]T}dz^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-toolbox",
   "metadata": {},
   "source": [
    "**Intuition**\n",
    "1. detect the edges.\n",
    "2. then compose the features together.\n",
    "\n",
    "**Parameter and hyperparameter**\n",
    "1. para: $w^{[i]}$ $b^{[i]}$\n",
    "2. hyper: $\\alpha$, #iterations, L, $n^{l}$, AF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-village",
   "metadata": {},
   "source": [
    "## 4 Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-uncle",
   "metadata": {},
   "source": [
    "### 4.1 Seperation of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-ecuador",
   "metadata": {},
   "source": [
    "Data is seperated into the following modes:\n",
    "\n",
    "**training ( train models )-- development set ( compare models ) -- test set ( test models )**,\n",
    "\n",
    "**training ( train models )-- development set ( compare models )**,\n",
    "\n",
    "1. When the data scale is large, e.g. over 1000000, the percentage 98:1:1 is fine.\n",
    "2. make sure the Dev/test set come from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-collective",
   "metadata": {},
   "source": [
    "### 4.2 Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-ozone",
   "metadata": {},
   "source": [
    "We shall sketch train set error and dev set error to see variance and bias.\n",
    "\n",
    "if the optimal error is 0 (i.e. human can tell the difference),\n",
    "1. 1/11 high variance (development set)\n",
    "2. 15/16 high bias (training set)\n",
    "3. 19/30 high bias & high variance\n",
    "\n",
    "And here are some things to fix the problems without huring the other direction.\n",
    "\n",
    "**Regularization/ more data -- reduce variance/overfit**\n",
    "\n",
    "**Large network/ better models -- stable bias/underfit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-computer",
   "metadata": {},
   "source": [
    "e.g. of $L_2$ regularization in neural network\n",
    "\n",
    "$$ J(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)},y)+\\frac{\\lambda}{2m}\\sum_{i}||w^{l}||_{2}^2\n",
    "$$\n",
    "$$w^{[l]}:=w^{[l]}-\\alpha dw^{[l]}\\textbf{, where } dw^{[l]}=old+\\frac{\\lambda}{m}\\omega^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-shepherd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
